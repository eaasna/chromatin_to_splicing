configfile: "config.yaml"

#Extract Nextera barcodes from the fastq files
rule extract_nextera_barcodes:
	input:
		f1 = "{dataset}/fastq/{sample}_1.fastq.gz",
		f2 = "{dataset}/fastq/{sample}_2.fastq.gz"
	output:
		f1 = "{dataset}/trimmed/{sample}.1.barcode.txt",
		f2 = "{dataset}/trimmed/{sample}.2.barcode.txt"
	resources:
		mem = 100
	threads: 1
	shell:
		"""
		module load python-3.6.0
		python /gpfs/hpchome/a72094/rocket/projects/chromatin_to_splicing/seqUtils/extractNexteraBarcode.py --fastq {input.f1} --type read1 > {output.f1}
		python /gpfs/hpchome/a72094/rocket/projects/chromatin_to_splicing/seqUtils/extractNexteraBarcode.py --fastq {input.f2} --type read2 > {output.f2}
		"""
		
#Trim adapters from fastq files
rule trim_adapters:
	input:
		fastq1 = "{dataset}/fastq/{sample}_1.fastq.gz",
		fastq2 = "{dataset}/fastq/{sample}_2.fastq.gz",
		barcode1 = "{dataset}/trimmed/{sample}.1.barcode.txt",
		barcode2 = "{dataset}/trimmed/{sample}.2.barcode.txt"
	params:
		prefix="{dataset}/trimmed/{sample}"
	resources:
		mem = 2000
	threads: 3
	output:
		"{dataset}/trimmed/{sample}-trimmed-pair1.fastq.gz",
		"{dataset}/trimmed/{sample}-trimmed-pair2.fastq.gz"
	script:
		"/gpfs/hpchome/a72094/rocket/projects/chromatin_to_splicing/seqUtils/trim_adapters.py"
				
#Rename trimmed fastq files from skewer
rule rename_trimmed_fastq:
	input:
		f1 = "{dataset}/trimmed/{sample}-trimmed-pair1.fastq.gz",
		f2 = "{dataset}/trimmed/{sample}-trimmed-pair2.fastq.gz"
	output:
		f1 = "{dataset}/trimmed/{sample}.1.trimmed.fastq.gz",
		f2 = "{dataset}/trimmed/{sample}.2.trimmed.fastq.gz"
	resources:
		mem = 100
	threads: 1
	shell:
		"mv {input.f1} {output.f1} && mv {input.f2} {output.f2}" 

#Align reads to the reference genome using BWA
rule align_reads:
	input:
		"{dataset}/trimmed/{sample}.1.trimmed.fastq.gz",
		"{dataset}/trimmed/{sample}.2.trimmed.fastq.gz"
	output:
		"{dataset}/aligned/{sample}.bam"
	params:
		genome = config["bwa_index"],
		rg="@RG\tID:{sample}\tSM:{sample}"
	resources:
		mem = 15000
	threads: 4
	shell:
		"""
		module load samtools-1.3
		module load bwa-0.7.12
		bwa mem -M -R '{params.rg}' -t {threads} {params.genome} {input} | samtools view -b - > {output}
		"""
		
#Sort BAM files by coordinates
rule sort_bams_by_position:
	input:
		"{dataset}/aligned/{sample}.bam"
	output:
		"{dataset}/aligned/{sample}.sorted.bam"
	resources:
		mem = 10000
	threads: 4
	shell:
		"""
		module load samtools-1.3
		samtools sort -T {wildcards.dataset}/aligned/{wildcards.sample} -O bam -@ {threads} {input} > {output}
		"""
#Index sorted bams
rule index_bams:
	input:
		"{dataset}/aligned/{sample}.sorted.bam"
	output:
		"{dataset}/aligned/{sample}.sorted.bam.bai"
	resources:
		mem = 100
	threads: 1
	shell:
		"""
		module load samtools-1.3
		samtools index {input}
		"""
		
#Keep only properly paired reads from nuclear chromosomes (remove MT)
rule filter_properly_paired:
	input:
		bam = "{dataset}/aligned/{sample}.sorted.bam",
		index = "{dataset}/aligned/{sample}.sorted.bam.bai"
	output:
		"{dataset}/filtered/{sample}.filtered.bam"
	resources:
		mem = 200
	threads: 1
	params:
		chr_list = "1 10 11 12 13 14 15 16 17 18 19 2 20 21 22 3 4 5 6 7 8 9 X Y"
	shell:
		"""
		module load samtools-1.3
		samtools view -h -b -f 2 {input.bam} {params.chr_list} > {output}
		"""
		
#Remove BWA entry from the BAM file header (conflicts with MarkDuplicates)
rule remove_bwa_header:
	input:
		"{dataset}/filtered/{sample}.filtered.bam"
	output:
		new_header = "{dataset}/filtered/{sample}.new_header.txt",
		bam = "{dataset}/filtered/{sample}.reheadered.bam"
	resources:
		mem = 100
	threads: 1
	shell:
		"""
		module load samtools-1.3
		samtools view -H {input} | grep -v 'ID:bwa' > {output.new_header}
		samtools reheader {output.new_header} {input} > {output.bam}
		"""

#Remove duplicates using Picard MarkDuplicates
rule remove_duplicates:
	input:
		"{dataset}/filtered/{sample}.reheadered.bam"
	output:
		bam = protected("{dataset}/filtered/{sample}.no_duplicates.bam"),
		metrics = "{dataset}/metrics/{sample}.MarkDuplicates.txt"
	resources:
		mem = 6000
	threads: 4
	shell:
		"""
		module load java-1.8.0_40 
		{config[picard_path]} MarkDuplicates I={input} O={output.bam} REMOVE_DUPLICATES=true METRICS_FILE= {output.metrics}
		"""
		
#Count the number of reads per chromosome (QC metric)
rule reads_per_chromosome:
	input:
		"{dataset}/aligned/{sample}.sorted.bam"
	output:
		"{dataset}/metrics/{sample}.chr_counts.txt"
	resources:
		mem = 200
	threads: 1
	shell:
		"""
		module load samtools-1.3
		samtools view {input} | cut -f3 | sort | uniq -c > {output}
		"""

#Sort BAM files by read name
rule sort_bams_by_name:
	input:
		"{dataset}/filtered/{sample}.no_duplicates.bam"
	output:
		temp("{dataset}/filtered/{sample}.sortedByName.bam")
	resources:
		mem = 10000
	threads: 4
	shell:
		"""
		module load samtools-1.3
		samtools sort -T {wildcards.dataset}/filtered/{wildcards.sample} -O bam -@ {threads} -n {input} > {output}
		"""
		
#Convert the bam file to a bed file of fragments
#First to bedpe and then to bed
rule bam_to_fragment_bed:
	input:
		"{dataset}/filtered/{sample}.sortedByName.bam"
	output:
		"{dataset}/bed/{sample}.fragments.bed.gz"
	resources:
		mem = 2000
	threads: 2
	shell:
		"""
		module load bedtools-2.24
		module load python-3.6.0
		bedtools bamtobed -bedpe -i {input} | python /gpfs/hpchome/a72094/rocket/projects/chromatin_to_splicing/seqUtils/bedpe2bed.py --maxFragmentLength 1000 | sort -k 1,1 | gzip > {output}
		"""
		
#Convert fragment bed files into cutsite bed files
rule fragments_to_cutsites_bed:
	input:
		"{dataset}/bed/{sample}.fragments.bed.gz"
	output:
		"{dataset}/bed/{sample}.cutsites.bed.gz"
	resources:
		mem = 2000
	threads: 2
	shell:
		"""
		module load python-3.6.0
		python /gpfs/hpchome/a72094/rocket/projects/chromatin_to_splicing/seqUtils/fragmentsToCutSites.py --bed {input} | sort -k1,1 | gzip > {output}
		"""

#Call peaks from cut site bed files
rule call_peaks:
	input:
		"{dataset}/bed/{sample}.cutsites.bed.gz"
	output:
		narrowPeak = "{dataset}/peaks/{sample}.narrowPeak.gz",
		summits = "{dataset}/peaks/{sample}.summits.bed.gz"
	resources:
		mem = 4000
	threads: 1
	params:
		outdir = "{dataset}/peaks/",
		xls = "{dataset}/peaks/{sample}_peaks.xls",
		temp_peaks = "{dataset}/peaks/{sample}_peaks.narrowPeak",
		temp_summits = "{dataset}/peaks/{sample}_summits.bed"
	shell:
		"""
		module load MACS-2.1.0
		macs2 callpeak --nomodel -t {input} --shift 25 --extsize 50 -q 0.01 -n {wildcards.sample} --outdir {params.outdir} -f BED -s 75
		rm {params.xls}
		mv {params.temp_peaks} {output.narrowPeak}
		mv {params.temp_summits} {output.summits}
		gzip {output.narrowPeak}
		gzip {output.summits}
		"""

#Count the number of occurences of each fragment length in the fragments bed file
rule count_fragment_lengths:
	input:
		"{dataset}/bed/{sample}.fragments.bed.gz"
	output:
		"{dataset}/metrics/{sample}.fragment_lengths.txt"
	resources:
		mem = 200
	threads: 1
	shell:
		"zcat {input} | cut -f5 | sort -n | uniq -c > {output}"

#Convert fragment bed file into bigwig
rule convert_bed_to_bigwig:
	input:
		"{dataset}/bed/{sample}.fragments.bed.gz"
	output:
		bigwig = "{dataset}/bigwig/{sample}.bw"
	params:
		bedgraph = "{dataset}/bigwig/{sample}.bg"
	resources:
		mem = 5000
	threads: 1
	shell:
		"""
		module load bedtools-2.24
		bedtools genomecov -bga -i {input} -g {config[chromosome_lengths]} > {params.bedgraph}
		bedGraphToBigWig {params.bedgraph} {config[chromosome_lengths]} {output.bigwig}
		rm {params.bedgraph}
		"""
		
#Make sure that all final output files get created
rule make_all:
	input:
		expand("{dataset}/bigwig/{sample}.bw", sample=config["samples"], dataset=config["path"]),
		expand("{dataset}/peaks/{sample}.narrowPeak.gz", sample=config["samples"], dataset=config["path"]),
		expand("{dataset}/metrics/{sample}.fragment_lengths.txt", sample=config["samples"], dataset=config["path"]),
		expand("{dataset}/metrics/{sample}.chr_counts.txt", sample=config["samples"], dataset=config["path"])
	output:
		"out.txt"
	resources:
		mem = 100
	threads: 1
	shell:
		"echo 'Done' > {output}"
		
# pÃ¤rast seda definePeaks.R 