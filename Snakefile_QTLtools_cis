import uuidimport osimport re#Sort bam files by name and quantify chromatin openness using featureCountsrule quantify_featureCounts:	input:		bam = "{dataset}/aligned/{sample}.bam",		peaks = "{dataset}/annotations/ATAC_joined_peaks.gtf"	output:		counts = "{dataset}/featureCounts/{sample}.featureCounts.txt",		summary = "{dataset}/featureCounts/{sample}.featureCounts.txt.summary"	params:		local_tmp = "/tmp/a72094_" + uuid.uuid4().hex + "/"	threads: 6	resources:		mem = 12000	run:		shell("mkdir {params.local_tmp}")		shell("rsync -aP --bwlimit=10000 {input.bam} {params.local_tmp}/{wildcards.sample}.bam")		shell("module load samtools-1.6 && samtools sort -n -m 1000M -o {params.local_tmp}/{wildcards.sample}.sorted.bam -O BAM --threads 5 {params.local_tmp}/{wildcards.sample}.bam")		shell("/gpfs/rocket/home/a72094/projects/chromatin_to_splicing/software/subread-1.6.2-source/bin/featureCounts -s0 -p -C -D 1000 -d 20 --donotsort -a {input.peaks} -o {output.counts} {params.local_tmp}/{wildcards.sample}.sorted.bam")		shell("rm -r {params.local_tmp}")#siin find_peaks_without_nearby_SNPs.R		rule remove_peaks_without_nearby_SNPs:    input:        counts = "{dataset}/featureCounts/{sample}.featureCounts.txt",        peaks = "{dataset}/peaks/peaks_with_tabs.txt"    output:        "{dataset}/featureCounts/{sample}.featureCounts.removed.txt"    threads: 1    resources:        mem = 7000    shell:        "grep -v -f {input.peaks} {input.counts} > {output}"		rule calculate_GC_content:	input:		fasta = "/gpfs/hpchome/a72094/rocket/annotations/GRCh38/Homo_sapiens.GRCh38.dna.primary_assembly.fa",		bed = "{dataset}/annotations/ATAC_joined_peaks.bed"	output:		"{dataset}/QC_measures/ATAC_GC_content.txt"	threads: 1	resources:		mem = 3000	shell:		"""		module load bedtools-2.24		bedtools nuc -fi {input.fasta} -bed {input.bed} > {output}		"""		rule generate_counts_bed:	output:		"{dataset}/counts/counts_matrix.bed"	threads: 1	resources:		mem = 2000	shell:		"""		module load R-3.3.0		Rscript --vanilla scripts/generate_genotype_bed.R		"""		rule calculate_CQN:	input:		counts = "{dataset}/counts/counts_matrix.bed",		gc = "{dataset}/QC_measures/ATAC_GC_content.txt"	output:		"{dataset}/normalized/counts_matrix_cqn.bed"	threads: 1	resources:		mem = 2000	shell:		"""		module load R-3.3.0		Rscript --vanilla scripts/normalize_CQN.R		"""rule calculate_FPKM:	input:		counts = "{dataset}/counts/counts_matrix.bed"	output:		"{dataset}/normalized/counts_matrix_fpkm.bed"	threads: 1	resources:		mem = 2000	shell:		"""		module load R-3.3.0		Rscript --vanilla scripts/normalize_fpkm.R		"""		#Sort normalized count file by SNP positionrule sort:	input:		"{dataset}/normalized/counts_matrix_{normalization}.bed"	output:		"{dataset}/normalized/counts_matrix_{normalization}.sorted.bed"	resources:		mem = 12000	threads: 20	shell:		"""		module load samtools-1.6		LANG=C sort --parallel=20 -k1,1n -k2,2n -k3,3n {input} > {output}		rm {input}		"""				# pole Ã¼ldse kindel kas see samm on vajalik, soovitan alguses proovida ilma?#sed -e '2~1s/^/chr/' {input} | bgzip > {output}rule add_chr:	input:		"{dataset}/normalized/counts_matrix_{normalization}.sorted.bed"	output:		"{dataset}/normalized/counts_matrix_{normalization}.bed.gz"	resources:		mem = 2000	threads: 1	shell:		"""		module load samtools-1.6		bgzip -c {input} > {output}		"""#columns 1, 2, 3 <- variant chr, start, end		#Tabix-index counts filesrule index:	input:		"{dataset}/normalized/counts_matrix_{normalization}.bed.gz"	output:		"{dataset}/normalized/counts_matrix_{normalization}.bed.gz.tbi"	resources:		mem = 4000	threads: 1	shell:		"""		module load samtools-1.6		tabix -s1 -b2 -e3 -f {input}		"""		rule phenotype_pca:	input:		bed = "{dataset}/normalized/counts_matrix_{normalization}.bed.gz",		tbi = "{dataset}/normalized/counts_matrix_{normalization}.bed.gz.tbi"	output:		"{dataset}/normalized/{normalization}.pca"	threads: 1	resources:		mem = 2000	shell:		"/gpfs/hpchome/a72094/software/bin/QTLtools pca --bed {input.bed} --scale --center --out {wildcards.dataset}/normalized/{wildcards.normalization}"rule genotype_pca:	input:		"{dataset}/genotypes/Kumasaka_100_samples.merged.vcf.gz"	output:		"{dataset}/genotypes/Kumasaka_100_samples.merged.pca"	threads: 1	resources:		mem = 2000	shell:		"/gpfs/hpchome/a72094/software/bin/QTLtools pca --vcf {input} --scale --center --maf 0.05 --distance 50000 --out {wildcards.dataset}/genotypes/Kumasaka_100_samples.merged"#add genotype and phenotype pca components into one file with pca_concat.R		i = list(range(1,100))	k = [100000]#k = [10000]#create covariate files for cis analysisrule generate_pca:	input:		"{dataset}/genotypes/Kumasaka_100_samples.merged.pca",		"{dataset}/normalized/{normalization}.pca"	output:		"{dataset}/covariates/{normalization}_{count}.pca"	threads: 1	resources:		mem = 2000	shell:		"""		module load R-3.3.0		Rscript --vanilla /gpfs/hpchome/evelin95/scripts/covariates.R		"""rule qtl_permutation:	input:		bed = "{dataset}/normalized/counts_matrix_{normalization}.bed.gz",		vcf = "{dataset}/genotypes/Kumasaka_100_samples.merged.vcf.gz",		pca = "{dataset}/covariates/{normalization}_{count}.pca"	output:		"{dataset}/QTL_rerun/{normalization}_permutations_{i}_100_{k}_pca_{count}.txt"	resources:		mem = 2000	threads: 1	shell:		"/gpfs/hpchome/a72094/software/bin/QTLtools cis --vcf {input.vcf} --bed {input.bed} --cov {input.pca} --permute 100 --chunk {wildcards.i} 100 --window {wildcards.k} --out {output}""""rule qtl_nominal:	input:		bed = "{dataset}/normalized/counts_matrix_{normalization}.bed.gz",		vcf = "{dataset}/genotypes/Kumasaka_100_samples.merged.vcf.gz",		pca = "{dataset}/normalized/{normalization}_vcf.pca"	output:		"{dataset}/QTL/{normalization}_nominal_{i}_100.txt"	resources:		mem = 2000	threads: 1	shell:		"/gpfs/hpchome/a72094/software/bin/QTLtools cis --vcf {input.vcf} --bed {input.bed} --cov {input.pca} --nominal 1 --chunk {wildcards.i} 100 --window 500000 --out {output} --normal""""rule make_all:	input:		expand("{dataset}/QTL_rerun/{normalization}_permutations_{i}_100_{k}_pca_{count}.txt", i=i, k=k, normalization=config["normalization"], dataset=config["path"], count=config["count"])		#expand("{dataset}/QTL/{normalization}_nominal_{i}_100.txt", i=i, normalization=config["normalization"], dataset=config["path"])	output:		"out.txt"	resources:		mem = 100	threads: 1	shell:		"echo 'Done' > {output}"		#merge 100 files with batch script merge.sh