import uuidimport osimport re#Sort bam files by name and quantify chromatin openness using featureCountsrule quantify_featureCounts:	input:		bam = "{dataset}/aligned/{sample}.bam",		peaks = "{dataset}/annotations/ATAC_joined_peaks.gtf"	output:		counts = "{dataset}/featureCounts/{sample}.featureCounts.txt",		summary = "{dataset}/featureCounts/{sample}.featureCounts.txt.summary"	params:		local_tmp = "/tmp/a72094_" + uuid.uuid4().hex + "/"	threads: 6	resources:		mem = 12000	run:		shell("mkdir {params.local_tmp}")		shell("rsync -aP --bwlimit=10000 {input.bam} {params.local_tmp}/{wildcards.sample}.bam")		shell("module load samtools-1.6 && samtools sort -n -m 1000M -o {params.local_tmp}/{wildcards.sample}.sorted.bam -O BAM --threads 5 {params.local_tmp}/{wildcards.sample}.bam")		shell("/gpfs/rocket/home/a72094/projects/chromatin_to_splicing/software/subread-1.6.2-source/bin/featureCounts -s0 -p -C -D 1000 -d 20 --donotsort -a {input.peaks} -o {output.counts} {params.local_tmp}/{wildcards.sample}.sorted.bam")		shell("rm -r {params.local_tmp}")#siin find_peaks_without_nearby_SNPs.R		rule remove_peaks_without_nearby_SNPs:    input:        counts = "{dataset}/featureCounts/{sample}.featureCounts.txt",        peaks = "{dataset}/peaks/peaks_with_tabs.txt"    output:        "{dataset}/featureCounts/{sample}.featureCounts.removed.txt"    threads: 1    resources:        mem = 7000    shell:        "grep -v -f {input.peaks} {input.counts} > {output}"		rule calculate_GC_content:	input:		fasta = "/gpfs/hpchome/a72094/rocket/annotations/GRCh38/Homo_sapiens.GRCh38.dna.primary_assembly.fa",		bed = "{dataset}/annotations/ATAC_joined_peaks.bed"	output:		"{dataset}/QC_measures/ATAC_GC_content.txt"	threads: 1	resources:		mem = 3000	shell:		"""		module load bedtools-2.24		bedtools nuc -fi {input.fasta} -bed {input.bed} > {output}		"""		rule generate_counts_bed:	output:		"{dataset}/counts/counts_matrix.bed"	threads: 1	resources:		mem = 2000	shell:		"""		module load R-3.3.0		Rscript --vanilla scripts/generate_genotype_bed.R		"""		rule calculate_CQN:	input:		counts = "{dataset}/counts/counts_matrix.bed",		gc = "{dataset}/QC_measures/ATAC_GC_content.txt"	output:		"{dataset}/normalized/counts_matrix_cqn.bed"	threads: 1	resources:		mem = 2000	shell:		"""		module load R-3.3.0		Rscript --vanilla scripts/normalize_CQN.R {input.counts} {input.gc} {output}		"""		#normaliseeritud counts faili indekseerimine enne pca analüüsirule calculate_FPKM:	input:		counts = "{dataset}/counts/counts_matrix.bed"	output:		"{dataset}/normalized/counts_matrix_fpkm.bed"	threads: 1	resources:		mem = 2000	shell:		"""		module load R-3.3.0		Rscript --vanilla scripts/normalize_fpkm.R {input.counts} {output}		"""		rule phenotype_pca_cqn:	input:		"{dataset}/normalized/counts_matrix_cqn.bed.gz"	output:		"{dataset}/normalized/counts_matrix_cqn.pca"	threads: 1	resources:		mem = 2000	shell:		"/gpfs/hpchome/a72094/software/bin/QTLtools pca --bed {input} --scale --center --out {wildcards.dataset}/normalized/counts_matrix_cqn"rule phenotype_pca_fpkm:	input:		"{dataset}/normalized/counts_matrix_fpkm.bed.gz"	output:		"{dataset}/normalized/counts_matrix_fpkm.pca"	threads: 1	resources:		mem = 2000	shell:		"/gpfs/hpchome/a72094/software/bin/QTLtools pca --bed {input} --scale --center --out {wildcards.dataset}/normalized/counts_matrix_fpkm"		rule genotype_pca:	input:		"{dataset}/genotypes/Kumasaka_100_samples.merged.vcf.gz"	output:		"{dataset}/genotypes/Kumasaka_100_samples.merged.pca"	threads: 1	resources:		mem = 2000	shell:		"/gpfs/hpchome/a72094/software/bin/QTLtools pca --vcf {input} --scale --center --maf 0.05 --distance 50000 --out {wildcards.dataset}/genotypes/Kumasaka_100_samples.merged"#add genotype and phenotype pca components into one file with pca_concat.R		i = list(range(1,100))	k = [10000, 100000]	rule qtl_permutation:	input:		bed = "{dataset}/normalized/counts_matrix_{normalization}.bed.gz",		vcf = "{dataset}/genotypes/Kumasaka_100_samples.merged.vcf.gz",		pca = "{dataset}/normalized/{normalization}_vcf.pca"	output:		"{dataset}/QTL/{normalization}_permutations_{i}_100_{k}.txt"	resources:		mem = 2000	threads: 1	shell:		"/gpfs/hpchome/a72094/software/bin/QTLtools cis --vcf {input.vcf} --bed {input.bed} --cov {input.pca} --permute 10000 --chunk {wildcards.i} 100 --window {wildcards.k} --out {output}"	rule qtl_nominal:	input:		bed = "{dataset}/normalized/counts_matrix_{normalization}.bed.gz",		vcf = "{dataset}/genotypes/Kumasaka_100_samples.merged.vcf.gz",		pca = "{dataset}/normalized/{normalization}_vcf.pca"	output:		"{dataset}/QTL/{normalization}_nominal_{i}_100.txt"	resources:		mem = 2000	threads: 1	shell:		"/gpfs/hpchome/a72094/software/bin/QTLtools cis --vcf {input.vcf} --bed {input.bed} --cov {input.pca} --nominal 1 --chunk {wildcards.i} 100 --window 500000 --out {output} --normal"		rule make_all:	input:		expand("{dataset}/QTL/{normalization}_permutations_{i}_100_{k}.txt", i=i, k=k, normalization=config["normalization"], dataset=config["path"]),		expand("{dataset}/QTL/{normalization}_nominal_{i}_100.txt", i=i, normalization=config["normalization"], dataset=config["path"])	output:		"out.txt"	resources:		mem = 100	threads: 1	shell:		"echo 'Done' > {output}"		#merge 100 files with batch script merge.sh